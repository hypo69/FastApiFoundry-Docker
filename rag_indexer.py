#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# =============================================================================
# –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞: RAG Indexer for FastAPI Foundry
# =============================================================================
# –û–ø–∏—Å–∞–Ω–∏–µ:
#   –°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
#   –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Markdown, HTML, TXT —Ñ–∞–π–ª—ã
#
# File: rag_indexer.py
# Project: AiStros
# Module: FastApiFoundry
# Author: hypo69
# License: CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
# Copyright: ¬© 2025 AiStros
# =============================================================================

import os
import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any
import numpy as np

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
try:
    from sentence_transformers import SentenceTransformer
    import faiss
except ImportError:
    print("‚ùå –¢—Ä–µ–±—É—é—Ç—Å—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install sentence-transformers faiss-cpu")
    exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RAGIndexer:
    """–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = None
        self.chunks = []
        self.embeddings = []
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    def read_file(self, file_path: Path) -> str:
        """–ü—Ä–æ—á–∏—Ç–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {file_path}: {e}")
            return ""
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # –ù–∞–π—Ç–∏ –±–ª–∏–∂–∞–π—à–∏–π —Ä–∞–∑—Ä—ã–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if end < len(text):
                # –ò—â–µ–º —Ç–æ—á–∫—É, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫
                for i in range(end, max(start + chunk_size // 2, end - 100), -1):
                    if text[i] in '.!?':
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
        
        return chunks
    
    def process_markdown(self, content: str, source: str) -> List[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å Markdown —Ñ–∞–π–ª"""
        chunks = []
        lines = content.split('\n')
        current_section = "Introduction"
        current_text = []
        
        for line in lines:
            line = line.strip()
            
            # –ó–∞–≥–æ–ª–æ–≤–∫–∏
            if line.startswith('#'):
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_text:
                    text = '\n'.join(current_text).strip()
                    if text:
                        for chunk in self.chunk_text(text):
                            chunks.append({
                                'source': source,
                                'section': current_section,
                                'text': chunk
                            })
                
                # –ù–æ–≤–∞—è —Å–µ–∫—Ü–∏—è
                current_section = line.lstrip('#').strip()
                current_text = []
            else:
                if line:  # –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    current_text.append(line)
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å–µ–∫—Ü–∏—è
        if current_text:
            text = '\n'.join(current_text).strip()
            if text:
                for chunk in self.chunk_text(text):
                    chunks.append({
                        'source': source,
                        'section': current_section,
                        'text': chunk
                    })
        
        return chunks
    
    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª"""
        content = self.read_file(file_path)
        if not content:
            return []
        
        source = file_path.name
        
        if file_path.suffix.lower() == '.md':
            return self.process_markdown(content, source)
        else:
            # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–∞–π–ª–æ–≤ - –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
            chunks = []
            for chunk in self.chunk_text(content):
                chunks.append({
                    'source': source,
                    'section': 'Content',
                    'text': chunk
                })
            return chunks
    
    def index_directory(self, docs_dir: Path) -> None:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""
        logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {docs_dir}")
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        supported_extensions = {'.md', '.txt', '.html', '.rst'}
        
        files_processed = 0
        for file_path in docs_dir.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {file_path}")
                chunks = self.process_file(file_path)
                self.chunks.extend(chunks)
                files_processed += 1
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {files_processed}")
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(self.chunks)}")
    
    def create_embeddings(self) -> None:
        """–°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤"""
        if not self.chunks:
            logger.error("‚ùå –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            return
        
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        texts = [chunk['text'] for chunk in self.chunks]
        
        # –°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏
        batch_size = 32
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = self.model.encode(batch, show_progress_bar=True)
            embeddings.extend(batch_embeddings)
        
        self.embeddings = np.array(embeddings).astype('float32')
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.embeddings.shape}")
    
    def build_faiss_index(self) -> faiss.Index:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å FAISS –∏–Ω–¥–µ–∫—Å"""
        logger.info("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        faiss.normalize_L2(self.embeddings)
        
        # –°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å
        dimension = self.embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ = –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        
        # –î–æ–±–∞–≤–∏—Ç—å –≤–µ–∫—Ç–æ—Ä—ã
        index.add(self.embeddings)
        
        logger.info(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
        return index
    
    def save_index(self, output_dir: Path) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ–∑–¥–∞—Ç—å FAISS –∏–Ω–¥–µ–∫—Å
        index = self.build_faiss_index()
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å FAISS –∏–Ω–¥–µ–∫—Å
        index_path = output_dir / "faiss.index"
        faiss.write_index(index, str(index_path))
        logger.info(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {index_path}")
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–æ–≤
        chunks_path = output_dir / "chunks.json"
        with open(chunks_path, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {chunks_path}")
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–Ω–¥–µ–∫—Å–µ
        info = {
            'model': self.model_name,
            'chunks_count': len(self.chunks),
            'dimension': self.embeddings.shape[1],
            'created_at': str(Path().cwd()),
            'version': '1.0.0'
        }
        
        info_path = output_dir / "index_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–Ω–¥–µ–∫—Å–µ: {info_path}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description="RAG Indexer –¥–ª—è FastAPI Foundry")
    
    parser.add_argument(
        '--docs-dir',
        type=str,
        required=True,
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default='./rag_index',
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: ./rag_index)"
    )
    
    parser.add_argument(
        '--model',
        type=str,
        default='sentence-transformers/all-MiniLM-L6-v2',
        help="–ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
    )
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    docs_dir = Path(args.docs_dir)
    if not docs_dir.exists():
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {docs_dir}")
        return
    
    output_dir = Path(args.output_dir)
    
    # –°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
    indexer = RAGIndexer(model_name=args.model)
    
    try:
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
        indexer.load_model()
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
        indexer.index_directory(docs_dir)
        
        if not indexer.chunks:
            logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            return
        
        # –°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        indexer.create_embeddings()
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å
        indexer.save_index(output_dir)
        
        logger.info("üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìÅ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_dir}")
        logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(indexer.chunks)}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        raise

if __name__ == "__main__":
    main()