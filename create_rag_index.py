# -*- coding: utf-8 -*-
# =============================================================================
# –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞: RAG Index Creator
# =============================================================================
# –û–ø–∏—Å–∞–Ω–∏–µ:
#   –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
#   –°–∫–∞–Ω–∏—Ä—É–µ—Ç .md —Ñ–∞–π–ª—ã –∏ —Å–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
#
# File: create_rag_index.py
# Project: FastApiFoundry (Docker)
# Version: 0.3.3
# Author: hypo69
# License: CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
# Copyright: ¬© 2025 AiStros
# Date: 9 –¥–µ–∫–∞–±—Ä—è 2025
# =============================================================================

import json
import logging
from pathlib import Path
from typing import List, Dict, Any
import numpy as np

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    RAG_AVAILABLE = True
except ImportError:
    print("‚ùå RAG dependencies not installed!")
    print("Install: pip install sentence-transformers faiss-cpu")
    exit(1)

from config_manager import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RAGIndexCreator:
    """–°–æ–∑–¥–∞—Ç–µ–ª—å RAG –∏–Ω–¥–µ–∫—Å–∞"""
    
    def __init__(self):
        self.model_name = config.rag_model
        self.chunk_size = config.rag_chunk_size
        self.index_dir = Path(config.rag_index_dir)
        self.model = None
        
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        logger.info(f"Loading embedding model: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        
    def scan_documents(self) -> List[Dict[str, Any]]:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞"""
        chunks = []
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
        docs_dir = Path("docs")
        if docs_dir.exists():
            for md_file in docs_dir.rglob("*.md"):
                chunks.extend(self._process_file(md_file, "docs"))
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º README —Ñ–∞–π–ª—ã
        for readme in Path(".").glob("README*.md"):
            chunks.extend(self._process_file(readme, "root"))
            
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –¥—Ä—É–≥–∏–µ –≤–∞–∂–Ω—ã–µ .md —Ñ–∞–π–ª—ã
        for md_file in Path(".").glob("*.md"):
            if md_file.name not in ["README.md"]:
                chunks.extend(self._process_file(md_file, "root"))
        
        logger.info(f"Found {len(chunks)} text chunks")
        return chunks
    
    def _process_file(self, file_path: Path, source_type: str) -> List[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª"""
        chunks = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏
            text_chunks = self._split_text(content)
            
            for i, chunk in enumerate(text_chunks):
                if len(chunk.strip()) > 50:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏
                    chunks.append({
                        'text': chunk.strip(),
                        'source': str(file_path),
                        'source_type': source_type,
                        'section': f"chunk_{i}",
                        'file_name': file_path.name
                    })
                    
        except Exception as e:
            logger.warning(f"Error processing {file_path}: {e}")
            
        return chunks
    
    def _split_text(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"""
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º –∏ —Ä–∞–∑–º–µ—Ä—É
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) < self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = para + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks
    
    def create_index(self, chunks: List[Dict[str, Any]]):
        """–°–æ–∑–¥–∞—Ç—å FAISS –∏–Ω–¥–µ–∫—Å"""
        logger.info("Creating embeddings...")
        
        # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç—ã
        texts = [chunk['text'] for chunk in chunks]
        
        # –°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = self.model.encode(texts, show_progress_bar=True)
        embeddings = np.array(embeddings).astype('float32')
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        faiss.normalize_L2(embeddings)
        
        # –°–æ–∑–¥–∞—Ç—å FAISS –∏–Ω–¥–µ–∫—Å
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
        index.add(embeddings)
        
        logger.info(f"Created FAISS index with {index.ntotal} vectors, dimension {dimension}")
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        index_path = self.index_dir / "faiss.index"
        meta_path = self.index_dir / "chunks.json"
        
        faiss.write_index(index, str(index_path))
        
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved index to {index_path}")
        logger.info(f"Saved metadata to {meta_path}")
        
        return index, chunks

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ Creating RAG Index for FastAPI Foundry")
    print("=" * 50)
    
    creator = RAGIndexCreator()
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
    creator.load_model()
    
    # –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
    chunks = creator.scan_documents()
    
    if not chunks:
        print("‚ùå No documents found to index!")
        return
    
    # –°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å
    index, chunks = creator.create_index(chunks)
    
    print(f"‚úÖ RAG Index created successfully!")
    print(f"   üìÅ Index directory: {creator.index_dir}")
    print(f"   üìä Total chunks: {len(chunks)}")
    print(f"   üîç Vector dimension: {index.d}")
    print(f"   üìà Total vectors: {index.ntotal}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
    print("\nüß™ Testing search...")
    test_query = "FastAPI configuration"
    query_vec = creator.model.encode([test_query])
    query_vec = np.array(query_vec).astype('float32')
    faiss.normalize_L2(query_vec)
    
    scores, indices = index.search(query_vec, 3)
    
    print(f"Query: '{test_query}'")
    for score, idx in zip(scores[0], indices[0]):
        if idx < len(chunks):
            chunk = chunks[idx]
            print(f"  üìÑ {chunk['file_name']} (score: {score:.3f})")
            print(f"     {chunk['text'][:100]}...")

if __name__ == "__main__":
    main()