#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# =============================================================================
# –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞: Model Management Client Example
# =============================================================================
# –û–ø–∏—Å–∞–Ω–∏–µ:
#   –ü—Ä–∏–º–µ—Ä –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º AI –º–æ–¥–µ–ª–µ–π
#   –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
#
# File: example_model_client.py
# Project: AiStros
# Module: FastApiFoundry
# Author: hypo69
# License: CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
# Copyright: ¬© 2025 AiStros
# =============================================================================

import asyncio
import json
from example_client import FastAPIFoundryClient

async def demo_model_management():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏"""
    
    async with FastAPIFoundryClient(
        base_url="http://localhost:8000",
        api_key=None  # API –∫–ª—é—á –æ—Ç–∫–ª—é—á–µ–Ω –≤ .env
    ) as client:
        
        print("ü§ñ FastAPI Foundry - Model Management Demo")
        print("=" * 60)
        
        # 1. –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        print("\n1Ô∏è‚É£ Available Providers:")
        providers = await client.get_model_providers()
        
        for provider in providers['providers']:
            print(f"  üì¶ {provider['name']} ({provider['provider_id']})")
            print(f"     {provider['description']}")
            print(f"     Features: {', '.join(provider['supported_features'])}")
            print(f"     API Key required: {'Yes' if provider['requires_api_key'] else 'No'}")
            if provider['default_endpoint']:
                print(f"     Default endpoint: {provider['default_endpoint']}")
            print()
        
        # 2. –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        print("\n2Ô∏è‚É£ Currently Connected Models:")
        connected = await client.get_connected_models()
        
        print(f"Total models: {connected['total_count']}")
        print(f"Online models: {connected['online_count']}")
        print(f"Default model: {connected.get('default_model', 'None')}")
        
        for model in connected['models']:
            status_emoji = "üü¢" if model['status'] == 'online' else "üî¥" if model['status'] == 'offline' else "‚ö™"
            print(f"  {status_emoji} {model['model_id']} ({model['provider']})")
            print(f"     Name: {model['model_name']}")
            print(f"     Status: {model['status']}")
            print(f"     Enabled: {model['enabled']}")
            print(f"     Usage: {model['usage_count']} times")
            if model['avg_response_time']:
                print(f"     Avg response: {model['avg_response_time']:.2f}s")
            print()
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–¥–æ—Ä–æ–≤—å–µ –º–æ–¥–µ–ª–µ–π
        print("\n3Ô∏è‚É£ Health Check:")
        health_result = await client.check_models_health()
        print(f"Health check: {'‚úÖ Completed' if health_result['success'] else '‚ùå Failed'}")
        
        # 4. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        print("\n4Ô∏è‚É£ Model Connection Examples:")
        
        # –ü—Ä–∏–º–µ—Ä –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è Ollama –º–æ–¥–µ–ª–∏
        print("\nüìù Example: Connect Ollama Model")
        print("Command:")
        print("  await client.connect_model(")
        print("      model_id='llama2:7b',")
        print("      provider='ollama',")
        print("      model_name='Llama 2 7B',")
        print("      endpoint_url='http://localhost:11434/api/'")
        print("  )")
        
        # –ü—Ä–∏–º–µ—Ä –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è OpenAI –º–æ–¥–µ–ª–∏
        print("\nüìù Example: Connect OpenAI Model")
        print("Command:")
        print("  await client.connect_model(")
        print("      model_id='gpt-3.5-turbo',")
        print("      provider='openai',")
        print("      model_name='GPT-3.5 Turbo',")
        print("      api_key='your-openai-api-key'")
        print("  )")
        
        # –ü—Ä–∏–º–µ—Ä –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è Anthropic –º–æ–¥–µ–ª–∏
        print("\nüìù Example: Connect Anthropic Model")
        print("Command:")
        print("  await client.connect_model(")
        print("      model_id='claude-3-sonnet-20240229',")
        print("      provider='anthropic',")
        print("      model_name='Claude 3 Sonnet',")
        print("      api_key='your-anthropic-api-key'")
        print("  )")
        
        # –ü—Ä–∏–º–µ—Ä –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏
        print("\nüìù Example: Connect Custom Model")
        print("Command:")
        print("  await client.connect_model(")
        print("      model_id='my-custom-model',")
        print("      provider='custom',")
        print("      model_name='My Custom AI Model',")
        print("      endpoint_url='http://my-server:8080/api/generate',")
        print("      api_key='optional-api-key'")
        print("  )")
        
        # 5. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç)
        print("\n5Ô∏è‚É£ Interactive Model Connection:")
        print("Would you like to connect a test model? (This is just a demo)")
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        test_model_data = {
            "model_id": "test-model-demo",
            "provider": "foundry",
            "model_name": "Test Demo Model",
            "endpoint_url": "http://localhost:55581/v1/",
            "enabled": True
        }
        
        print(f"\nüîÑ Connecting test model: {test_model_data['model_id']}")
        try:
            result = await client.connect_model(**test_model_data)
            if result['success']:
                print(f"‚úÖ {result['message']}")
            else:
                print(f"‚ùå {result['message']}")
                if result.get('error'):
                    print(f"   Error: {result['error']}")
        except Exception as e:
            print(f"‚ùå Connection failed: {e}")
        
        # 6. –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        print("\n6Ô∏è‚É£ Updated Model List:")
        updated_connected = await client.get_connected_models()
        print(f"Total models: {updated_connected['total_count']}")
        
        # 7. –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
        print("\n7Ô∏è‚É£ Useful Commands:")
        print("üìã List all endpoints:")
        print("  curl http://localhost:8000/")
        print()
        print("üîç Get connected models:")
        print("  curl http://localhost:8000/api/v1/models/connected")
        print()
        print("üì¶ Get providers:")
        print("  curl http://localhost:8000/api/v1/models/providers")
        print()
        print("üîó Connect new model:")
        print("  curl -X POST http://localhost:8000/api/v1/models/connect \\")
        print("    -H 'Content-Type: application/json' \\")
        print("    -d '{\"model_id\": \"new-model\", \"provider\": \"foundry\"}'")
        print()
        print("üß™ Test model:")
        print("  curl -X POST http://localhost:8000/api/v1/models/test-model-demo/test \\")
        print("    -H 'Content-Type: application/json' \\")
        print("    -d '{\"test_prompt\": \"Hello, world!\"}'")
        print()
        print("‚ùå Disconnect model:")
        print("  curl -X DELETE http://localhost:8000/api/v1/models/test-model-demo")
        
        print("\n" + "=" * 60)
        print("üéâ Model Management Demo Complete!")
        print("üìö Check /docs for full API documentation")

if __name__ == "__main__":
    asyncio.run(demo_model_management())