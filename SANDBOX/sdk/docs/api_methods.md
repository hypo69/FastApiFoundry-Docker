# üì° API Methods Reference

–ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ FoundryClient.

---

## üè• Health & Status

### `health() -> HealthStatus`

–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã.

```python
health = client.health()
print(f"API: {health.status}")
print(f"Foundry: {health.foundry_status}")
print(f"RAG chunks: {health.rag_chunks}")
```

### `is_alive() -> bool`

–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ API.

```python
if client.is_alive():
    print("API –¥–æ—Å—Ç—É–ø–µ–Ω")
```

### `wait_for_ready(max_wait=60) -> bool`

–ñ–¥–∞—Ç—å –ø–æ–∫–∞ API —Å—Ç–∞–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–º.

```python
if client.wait_for_ready(max_wait=30):
    print("API –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
```

---

## ü§ñ Models Management

### `list_models() -> List[ModelInfo]`

–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

```python
models = client.list_models()
for model in models:
    print(f"{model.id} ({model.provider})")
```

### `get_connected_models() -> List[ModelInfo]`

–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏.

```python
connected = client.get_connected_models()
print(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(connected)}")
```

### `load_model(model_id: str) -> bool`

–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –≤ Foundry.

```python
success = client.load_model("deepseek-r1-distill-qwen-7b-generic-cpu:3")
if success:
    print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
```

### `unload_model(model_id: str) -> bool`

–í—ã–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ Foundry.

```python
success = client.unload_model("model-id")
```

### `get_model_status(model_id: str) -> Dict`

–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏.

```python
status = client.get_model_status("model-id")
print(status.get("status", "unknown"))
```

---

## ‚úçÔ∏è Text Generation

### `generate(prompt, **kwargs) -> GenerationResponse`

–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `prompt: str` - –í—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–º–ø—Ç
- `model: Optional[str]` - ID –º–æ–¥–µ–ª–∏
- `temperature: Optional[float]` - –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (0.0-2.0)
- `max_tokens: Optional[int]` - –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
- `use_rag: bool = True` - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RAG
- `system_prompt: Optional[str]` - –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç

```python
response = client.generate(
    prompt="–û–±—ä—è—Å–Ω–∏ –∫–≤–∞–Ω—Ç–æ–≤—É—é —Ñ–∏–∑–∏–∫—É",
    temperature=0.7,
    max_tokens=500,
    use_rag=True
)

if response.success:
    print(response.content)
    print(f"–ú–æ–¥–µ–ª—å: {response.model_used}")
    print(f"–¢–æ–∫–µ–Ω–æ–≤: {response.tokens_used}")
```

### `chat(message, **kwargs) -> GenerationResponse`

–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —á–∞—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–µ—Å—Å–∏–π.

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `message: str` - –°–æ–æ–±—â–µ–Ω–∏–µ
- `conversation_id: Optional[str]` - ID —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
- `model: Optional[str]` - ID –º–æ–¥–µ–ª–∏
- `use_rag: bool = True` - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RAG

```python
response = client.chat(
    message="–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?",
    conversation_id="session-123",
    use_rag=True
)
```

### `batch_generate(prompts, **kwargs) -> List[GenerationResponse]`

–ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤.

```python
prompts = [
    "–ß—Ç–æ —Ç–∞–∫–æ–µ AI?",
    "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç ML?",
    "–û–±—ä—è—Å–Ω–∏ Deep Learning"
]

responses = client.batch_generate(
    prompts=prompts,
    max_tokens=100,
    use_rag=True
)

for i, response in enumerate(responses):
    print(f"–û—Ç–≤–µ—Ç {i+1}: {response.content}")
```

---

## üîç RAG System

### `rag_search(query: str, top_k=5) -> List[Dict]`

–ü–æ–∏—Å–∫ –≤ RAG –∏–Ω–¥–µ–∫—Å–µ.

```python
results = client.rag_search("Docker installation", top_k=3)
for result in results:
    print(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {result['source']}")
    print(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['score']:.3f}")
    print(f"–¢–µ–∫—Å—Ç: {result['text'][:100]}...")
```

### `rag_status() -> Dict`

–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å RAG —Å–∏—Å—Ç–µ–º—ã.

```python
status = client.rag_status()
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {status.get('loaded', False)}")
print(f"–ß–∞–Ω–∫–æ–≤: {status.get('chunks_count', 0)}")
```

### `rag_clear() -> bool`

–û—á–∏—Å—Ç–∏—Ç—å RAG –∏–Ω–¥–µ–∫—Å.

```python
if client.rag_clear():
    print("RAG –∏–Ω–¥–µ–∫—Å –æ—á–∏—â–µ–Ω")
```

### `rag_reload() -> bool`

–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å RAG –∏–Ω–¥–µ–∫—Å.

```python
if client.rag_reload():
    print("RAG –∏–Ω–¥–µ–∫—Å –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω")
```

### `rag_initialize() -> bool`

–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å RAG —Å–∏—Å—Ç–µ–º—É.

```python
if client.rag_initialize():
    print("RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
```

---

## ‚öôÔ∏è Configuration

### `get_config() -> Dict`

–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å–∏—Å—Ç–µ–º—ã.

```python
config = client.get_config()
foundry_config = config.get("foundry_ai", {})
print(f"Foundry URL: {foundry_config.get('base_url')}")
```

### `update_config(config: Dict) -> bool`

–û–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å–∏—Å—Ç–µ–º—ã.

```python
new_config = {
    "foundry_ai": {
        "default_model": "new-model-id"
    }
}

if client.update_config(new_config):
    print("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
```

### `set_default_model(model_id: str) -> bool`

–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.

```python
if client.set_default_model("deepseek-r1-distill-qwen-7b-generic-cpu:3"):
    print("–ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
```

---

## üè≠ Foundry Management

### `foundry_status() -> Dict`

–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å Foundry —Å–µ—Ä–≤–∏—Å–∞.

```python
status = client.foundry_status()
print(f"–°—Ç–∞—Ç—É—Å: {status.get('status')}")
print(f"URL: {status.get('url')}")
```

### `foundry_models_loaded() -> List[Dict]`

–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ Foundry.

```python
loaded = client.foundry_models_loaded()
for model in loaded:
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞: {model.get('id')}")
```

---

## üìä Monitoring & Logs

### `get_logs(level=None, limit=100) -> List[Dict]`

–ü–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ª–æ–≥–∏.

```python
# –í—Å–µ –ª–æ–≥–∏
logs = client.get_logs(limit=50)

# –¢–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏
errors = client.get_logs(level="error", limit=20)

for log in errors:
    print(f"[{log['level']}] {log['message']}")
```

### `get_metrics() -> Dict`

–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

```python
metrics = client.get_metrics()
print(f"–í—Å–µ–≥–æ –ª–æ–≥–æ–≤: {metrics.get('total_logs', 0)}")
print(f"–û—à–∏–±–æ–∫: {metrics.get('errors', 0)}")
```

---

## üîß Utility Methods

### `test_connection() -> Dict`

–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ API.

```python
result = client.test_connection()
if result["connected"]:
    print(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ OK ({result['response_time']:.3f}s)")
else:
    print(f"–û—à–∏–±–∫–∞: {result['error']}")
```

### `auto_setup() -> Dict`

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã.

```python
setup_result = client.auto_setup()
print(f"Health check: {setup_result['health_check']}")
print(f"Models loaded: {setup_result['models_loaded']}")
print(f"RAG initialized: {setup_result['rag_initialized']}")
```

### `quick_test(prompt="Hello") -> Dict`

–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –≤—Å–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π.

```python
test_result = client.quick_test("Test prompt")
print(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ: {test_result['connection']}")
print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {test_result['generation']}")
print(f"RAG –ø–æ–∏—Å–∫: {test_result['rag_search']}")
print(f"–ú–æ–¥–µ–ª–µ–π: {test_result['models']}")
```

---

## üöÄ Advanced Methods

### `generate_with_retry(prompt, max_retries=3, **kwargs)`

–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏.

```python
response = client.generate_with_retry(
    prompt="–°–ª–æ–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å",
    max_retries=5,
    max_tokens=200
)
```

### `smart_generate(prompt, prefer_rag=True, **kwargs)`

–£–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

```python
response = client.smart_generate(
    prompt="–†–∞—Å—Å–∫–∞–∂–∏ –æ –ø—Ä–æ–µ–∫—Ç–µ",
    prefer_rag=True,
    fallback_model="backup-model"
)
```

---

**FastAPI Foundry SDK v0.2.1**  
¬© 2025 AiStros Team