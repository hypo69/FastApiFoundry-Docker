# ğŸ§  Models API

---
**ğŸ“š ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ:** [ğŸ  Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ](../README.md) | [ğŸ“¡ API](README.md) | [ğŸ¤– Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ](generation.md) | [ğŸ§  ĞœĞ¾Ğ´ĞµĞ»Ğ¸](models.md) | [ğŸ” RAG](rag.md) | [ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³](monitoring.md)
---

## GET `/api/v1/models`

**Response:**
```json
{
  "success": true,
  "models": [
    {
      "id": "deepseek-chat",
      "name": "DeepSeek Chat",
      "provider": "foundry",
      "status": "online"
    }
  ],
  "total_count": 1,
  "online_count": 1
}
```

## GET `/api/v1/models/connected`

**Response:**
```json
{
  "success": true,
  "models": [
    {
      "model_id": "deepseek-chat",
      "provider": "foundry",
      "status": "online",
      "usage_count": 42,
      "avg_response_time": 1.23
    }
  ],
  "total_count": 1,
  "online_count": 1
}
```

## POST `/api/v1/models/connect`

**Request:**
```json
{
  "model_id": "llama-3.2-1b",
  "provider": "foundry",
  "model_name": "Llama 3.2 1B",
  "endpoint_url": "http://localhost:55581/v1/",
  "enabled": true
}
```

## GET `/api/v1/models/providers`

**Response:**
```json
{
  "success": true,
  "providers": [
    {
      "provider_id": "foundry",
      "name": "Foundry AI",
      "requires_api_key": false,
      "supported_features": ["text_generation", "chat"]
    }
  ]
}
```

---
**[â¬†ï¸ ĞĞ°Ğ·Ğ°Ğ´ Ğº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ API](README.md)**