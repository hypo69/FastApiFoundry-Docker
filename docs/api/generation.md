# ü§ñ Text Generation API

---
**üìö –ù–∞–≤–∏–≥–∞—Ü–∏—è:** [üè† –ì–ª–∞–≤–Ω–∞—è](../README.md) | [üì° API](README.md) | [ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è](generation.md) | [üß† –ú–æ–¥–µ–ª–∏](models.md) | [üîç RAG](rag.md) | [üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥](monitoring.md)
---

## POST `/api/v1/generate`

**Request:**
```json
{
  "prompt": "–ß—Ç–æ —Ç–∞–∫–æ–µ FastAPI Foundry?",
  "model": "deepseek-chat",
  "temperature": 0.7,
  "max_tokens": 2048,
  "use_rag": true,
  "system_prompt": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞"
}
```

**Response:**
```json
{
  "success": true,
  "content": "FastAPI Foundry - —ç—Ç–æ REST API —Å–µ—Ä–≤–µ—Ä...",
  "model": "deepseek-chat",
  "tokens_used": 150,
  "rag_context_used": true,
  "generation_time": 2.34
}
```

## POST `/api/v1/batch-generate`

**Request:**
```json
{
  "prompts": ["–í–æ–ø—Ä–æ—Å 1", "–í–æ–ø—Ä–æ—Å 2"],
  "model": "deepseek-chat",
  "temperature": 0.7,
  "max_tokens": 1000,
  "use_rag": true
}
```

**Response:**
```json
{
  "success": true,
  "results": [
    {
      "prompt": "–í–æ–ø—Ä–æ—Å 1",
      "content": "–û—Ç–≤–µ—Ç 1...",
      "tokens_used": 75
    }
  ],
  "total_tokens": 157,
  "processing_time": 4.12
}
```

---
**[‚¨ÜÔ∏è –ù–∞–∑–∞–¥ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ API](README.md)**