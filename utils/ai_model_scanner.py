# -*- coding: utf-8 -*-
# =============================================================================
# –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞: –ü–æ–∏—Å–∫ –≤—Å–µ—Ö AI –º–æ–¥–µ–ª–µ–π –≤ —Å–∏—Å—Ç–µ–º–µ
# =============================================================================
# –û–ø–∏—Å–∞–Ω–∏–µ:
#   –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π Foundry, Ollama, HuggingFace –∏ –¥—Ä—É–≥–∏—Ö AI —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
#
# File: ai_model_scanner.py
# Project: FastApiFoundry (Docker)
# Version: 0.2.1
# Author: hypo69
# License: CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
# Copyright: ¬© 2025 AiStros
# Date: 9 –¥–µ–∫–∞–±—Ä—è 2025
# =============================================================================

import os
import sys
from pathlib import Path
from typing import List, Dict, Any
import json

def scan_all_ai_models() -> Dict[str, Any]:
    """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö AI –º–æ–¥–µ–ª–µ–π –≤ —Å–∏—Å—Ç–µ–º–µ"""
    
    results = {
        "foundry": {"directories": [], "models": []},
        "ollama": {"directories": [], "models": []},
        "huggingface": {"directories": [], "models": []},
        "other": {"directories": [], "models": []}
    }
    
    # –ü—É—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
    search_locations = {
        "foundry": [
            Path.home() / ".foundry",
            Path.home() / ".cache" / "foundry",
            Path("C:") / "foundry" if sys.platform == "win32" else Path("/opt/foundry"),
        ],
        "ollama": [
            Path.home() / ".ollama",
            Path("C:") / "Users" / os.getenv("USERNAME", "") / ".ollama" if sys.platform == "win32" else Path.home() / ".ollama",
            Path("/usr/share/ollama") if sys.platform != "win32" else None,
        ],
        "huggingface": [
            Path.home() / ".cache" / "huggingface",
            Path.home() / ".cache" / "transformers",
            Path.home() / "transformers_cache",
        ]
    }
    
    print("üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ AI –º–æ–¥–µ–ª–µ–π –≤ —Å–∏—Å—Ç–µ–º–µ...")
    print()
    
    for framework, paths in search_locations.items():
        print(f"üîç –ü–æ–∏—Å–∫ {framework.upper()} –º–æ–¥–µ–ª–µ–π...")
        
        for path in paths:
            if path and path.exists():
                print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {path}")
                results[framework]["directories"].append(str(path))
                
                # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
                try:
                    for item in path.rglob("*"):
                        if item.is_file():
                            size_mb = item.stat().st_size / (1024 * 1024)
                            
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏
                            if any(ext in item.name.lower() for ext in [
                                '.bin', '.safetensors', '.gguf', '.ggml', 
                                'pytorch_model', 'model.json', 'config.json',
                                '.pt', '.pth', '.onnx'
                            ]):
                                if size_mb > 5:  # –§–∞–π–ª—ã –±–æ–ª—å—à–µ 5MB
                                    results[framework]["models"].append({
                                        "name": item.name,
                                        "path": str(item),
                                        "size_mb": round(size_mb, 1),
                                        "parent": item.parent.name
                                    })
                except PermissionError:
                    print(f"  ‚ö†Ô∏è  –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ {path}")
            else:
                print(f"  ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞: {path}")
        print()
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –æ–±—â–∏—Ö –º–µ—Å—Ç–∞—Ö
    print("üîç –ü–æ–∏—Å–∫ –≤ –æ–±—â–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö...")
    common_paths = [
        Path.home() / "models",
        Path.home() / "Downloads",
        Path("C:") / "models" if sys.platform == "win32" else Path("/models"),
        Path(".") / "models",
    ]
    
    for path in common_paths:
        if path.exists():
            print(f"  ‚úÖ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {path}")
            try:
                for item in path.rglob("*"):
                    if item.is_file():
                        size_mb = item.stat().st_size / (1024 * 1024)
                        if size_mb > 50 and any(ext in item.name.lower() for ext in [
                            '.bin', '.safetensors', '.gguf', '.ggml'
                        ]):
                            results["other"]["models"].append({
                                "name": item.name,
                                "path": str(item),
                                "size_mb": round(size_mb, 1),
                                "parent": item.parent.name
                            })
            except PermissionError:
                print(f"  ‚ö†Ô∏è  –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ {path}")
    
    return results

def check_ai_installations() -> Dict[str, bool]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö AI —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤"""
    
    installations = {}
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥—ã
    commands = {
        "foundry": ["foundry", "foundry-server"],
        "ollama": ["ollama"],
        "python": ["python", "python3"],
        "transformers": ["transformers-cli"],
    }
    
    import shutil
    
    for framework, cmds in commands.items():
        found = False
        for cmd in cmds:
            if shutil.which(cmd):
                installations[framework] = True
                found = True
                break
        if not found:
            installations[framework] = False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Python –ø–∞–∫–µ—Ç–æ–≤
    try:
        import transformers
        installations["transformers_lib"] = True
    except ImportError:
        installations["transformers_lib"] = False
    
    try:
        import torch
        installations["pytorch"] = True
    except ImportError:
        installations["pytorch"] = False
    
    return installations

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 70)
    print("ü§ñ –°–ö–ê–ù–ï–† AI –ú–û–î–ï–õ–ï–ô –í –°–ò–°–¢–ï–ú–ï")
    print("=" * 70)
    print()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–æ–∫
    installations = check_ai_installations()
    
    print("üîß –£–°–¢–ê–ù–û–í–õ–ï–ù–ù–´–ï –§–†–ï–ô–ú–í–û–†–ö–ò:")
    for framework, installed in installations.items():
        status = "‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω" if installed else "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω"
        print(f"  {framework}: {status}")
    print()
    
    # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    results = scan_all_ai_models()
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("=" * 70)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 70)
    
    total_models = 0
    for framework, data in results.items():
        model_count = len(data["models"])
        total_models += model_count
        
        if model_count > 0:
            print(f"\nü§ñ {framework.upper()}: {model_count} –º–æ–¥–µ–ª–µ–π")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –º–æ–¥–µ–ª–µ–π
            for model in data["models"][:5]:
                print(f"  üìÑ {model['name']} ({model['size_mb']} MB)")
                print(f"     üìÅ {model['parent']}")
            
            if model_count > 5:
                print(f"     ... –∏ –µ—â–µ {model_count - 5} –º–æ–¥–µ–ª–µ–π")
    
    print(f"\nüìä –í–°–ï–ì–û –ù–ê–ô–î–ï–ù–û –ú–û–î–ï–õ–ï–ô: {total_models}")
    
    if total_models == 0:
        print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print("  1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama: https://ollama.ai/")
        print("  2. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏: ollama pull llama2")
        print("  3. –ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Foundry —Å –º–æ–¥–µ–ª—è–º–∏")
        print("  4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ Downloads –Ω–∞ –Ω–∞–ª–∏—á–∏–µ .gguf —Ñ–∞–π–ª–æ–≤")

if __name__ == "__main__":
    main()